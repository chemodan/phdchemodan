{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the collected tweets from CSVs and analyze them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "from sklearn import decomposition\n",
    "import statsmodels.regression.linear_model as lm\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "import requests\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "from PIL import Image\n",
    "\n",
    "import itertools\n",
    "import base64\n",
    "\n",
    "MYDIR = \"./\"\n",
    "\n",
    "COUNTIES = [\"Alameda\", \"San Francisco\", \"San Mateo\", \"Santa Clara\", \"Santa Cruz\"]\n",
    "\n",
    "BIG_TOPICS = {\"race\": [\"black\",\"blm\",\"racism\", \"race\", \"racial\"],\n",
    "              \"wildfires\": [\"fires\", \"wildfires\", \"wild fires\", \"air quality\", \"evacuation\"],\n",
    "              \"politics\": [\"trump\", \"biden\", \"elections\", \"democrat\", \"liberal\", \"republican\", \"conservative\"],\n",
    "              \"covid\": [\"covid\", \"coronavirus\", \"corona\", \"pandemic\", \"virus\", \"CDC\", \"WHO\", \"fauci\"]\n",
    "             }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TweetAnalyzer:\n",
    "    \n",
    "    def __init__(self, \n",
    "                 counties: List[str],\n",
    "                 big_topics: Dict[str, object],\n",
    "                 state_id: str = \"CA\"\n",
    "                ):\n",
    "        \n",
    "        \"\"\"\n",
    "        Get the files for analysis\n",
    "        \"\"\"\n",
    "        self.mMyDir = MYDIR\n",
    "        \n",
    "        allFiles = os.listdir(self.mMyDir)\n",
    "        self.mMyFiles = [ff for ff in allFiles if 'twdf' in ff and '.csv' in ff]\n",
    "        self.mMyFiles\n",
    "\n",
    "        self.mTopics = []\n",
    "\n",
    "        \"\"\"\n",
    "        Set the counties and topics of interest for search\n",
    "        \"\"\"\n",
    "        if counties is None:\n",
    "            self.mCounties = None\n",
    "        else:\n",
    "            self.mCounties = counties.copy()\n",
    "        self.mStateID = state_id\n",
    "        \n",
    "        if big_topics is None:\n",
    "            self.mBigTopics = None\n",
    "        else:\n",
    "            self.mBigTopics = big_topics.copy()\n",
    "            for kk in self.mBigTopics.keys():\n",
    "                self.mTopics.extend(self.mBigTopics[kk])\n",
    "\n",
    "        \"\"\"\n",
    "        Identify the cities in each county\n",
    "        \"\"\"\n",
    "        self.mCityCountyDF = pd.read_csv(self.mMyDir + \\\n",
    "                            \"../data/simplemaps_uscities_basicv1.7/uscities.csv\")\n",
    "\n",
    "        if self.mCounties is None:\n",
    "            self.mCiCounties = None\n",
    "            self.mCiStates = None\n",
    "            self.mCities = None\n",
    "            self.mStateName = None\n",
    "            self.mCountyTopics = None\n",
    "            \n",
    "        else:\n",
    "            city_county = self.getCitiesForCounties()\n",
    "\n",
    "           \n",
    "\n",
    "            self.mCiCounties = list(city_county.sort_values(\"city_county\")[\"city_county\"].unique())\n",
    "            self.mCiStates = list(city_county.sort_values(\"city_state\")[\"city_state\"].unique())\n",
    "            self.mCities = list(city_county.sort_values(\"city\")[\"city\"].unique())\n",
    "\n",
    "            self.mCountyTopics = []\n",
    "            #         self.mCountyTopics.extend(self.mCiCounties.copy())\n",
    "            self.mCountyTopics.extend(self.mCities.copy())\n",
    "            self.mCountyTopics.extend(self.mTopics.copy())\n",
    "\n",
    "            for cc in self.mCities:\n",
    "                for tt in self.mTopics:\n",
    "                    ct = cc + \", \" + tt\n",
    "                    self.mCountyTopics.append(ct)\n",
    "        \n",
    "        \"\"\"\n",
    "        Get the self.mAPI information\n",
    "        \"\"\"\n",
    "        api_key, api_secret_key, bearer_token = \\\n",
    "            self.get_api_keys_and_bearer_token()\n",
    "\n",
    "        self.mAPI: Dict[str, str] = {\"key\": api_key,\n",
    "                                     \"secret_key\": api_secret_key,\n",
    "                                     \"bearer_token\": bearer_token\n",
    "                                    }\n",
    "        \n",
    "        \"\"\"\n",
    "        set the self.mSearchParams\n",
    "        \"\"\"\n",
    "        search_url, search_headers = \\\n",
    "            self.get_search_url_search_headers(api_key, \n",
    "                                               api_secret_key)\n",
    "            \n",
    "        self.mSearchParams = {\"url\": search_url,\n",
    "                              \"headers\": search_headers\n",
    "                             }\n",
    "        \n",
    "        self.mMeltwaterQuery = \\\n",
    "            self.formulateQuery4meltwater(self.mStateName)\n",
    "        \n",
    "        \n",
    "    \"\"\"\n",
    "    Generate the search query for Meltwater's UI\n",
    "    \"\"\"\n",
    "    def formulateQuery4meltwater(self, state):\n",
    "        \n",
    "        if self.mCounties is None:\n",
    "            return None\n",
    "        \n",
    "        counties_cities = self.mCounties.copy()\n",
    "        counties_cities.extend(self.mCities)\n",
    "        counties_cities\n",
    "        \n",
    "        retQueryString = '(\"' + state + \\\n",
    "            '\") AND ((\"' +\\\n",
    "            '\") OR (\"'.join(counties_cities) + '\"))'\n",
    "        \n",
    "        return retQueryString\n",
    "        \n",
    "    \"\"\"\n",
    "    Using the simplemaps us cities data, get the cities in each \n",
    "    county in self.mCounties within self.mStateID\n",
    "    \"\"\"\n",
    "    def getCitiesForCounties(self, verbose: bool = False) -> pd.DataFrame:\n",
    "        \n",
    "        cities_counties = self.mCityCountyDF.copy()\n",
    "        if verbose:\n",
    "            print(cities_counties.head())\n",
    "        whatIneed = cities_counties[[\"city\", \n",
    "                                     \"county_name\", \n",
    "                                     \"state_id\", \n",
    "                                     \"state_name\"]].copy()\n",
    "        \n",
    "        mystate = whatIneed.loc[whatIneed.state_id == self.mStateID].copy()\n",
    "        myCounties = \\\n",
    "            mystate.loc[mystate.county_name.isin(self.mCounties)].copy()\n",
    "            \n",
    "        myCounties[\"city_county\"] = myCounties[\"city\"] + \", \" + myCounties[\"county_name\"]\n",
    "        myCounties[\"city_state\"] = myCounties[\"city\"] + \", \" + myCounties[\"state_name\"]\n",
    "        \n",
    "        return myCounties.reset_index(drop=True)\n",
    "        \n",
    "    \n",
    "    def get_tweets(self, api=None, screen_name=None):\n",
    "        timeline = api.GetUserTimeline(screen_name=screen_name, count=200)\n",
    "        earliest_tweet = min(timeline, key=lambda x: x.id).id\n",
    "        print(\"getting tweets before:\", earliest_tweet)\n",
    "\n",
    "        while True:\n",
    "            tweets = api.GetUserTimeline(\n",
    "                screen_name=screen_name, max_id=earliest_tweet, count=200\n",
    "            )\n",
    "            new_earliest = min(tweets, key=lambda x: x.id).id\n",
    "\n",
    "            if not tweets or new_earliest == earliest_tweet:\n",
    "                break\n",
    "            else:\n",
    "                earliest_tweet = new_earliest\n",
    "                print(\"getting tweets before:\", earliest_tweet)\n",
    "                timeline += tweets\n",
    "\n",
    "        return timeline\n",
    "\n",
    "    def get_api_keys_and_bearer_token(self, verbose: bool = False) -> List[str]:\n",
    "\n",
    "        df = pd.read_csv(\"./chemodan123KKs.csv\")\n",
    "\n",
    "        if verbose:\n",
    "            print(df)\n",
    "\n",
    "        keyDF = df.loc[df.name == 'api_key'].copy()\n",
    "        api_key = list(keyDF[\"value\"])[0]\n",
    "\n",
    "        sKeyDF = df.loc[df.name == 'api_secret_key'].copy()\n",
    "        api_secret_key = list(sKeyDF[\"value\"])[0]\n",
    "\n",
    "        bTokDF = df.loc[df.name == 'bearer_token'].copy()\n",
    "        bearer_token = list(bTokDF[\"value\"])[0]\n",
    "\n",
    "        return api_key, api_secret_key, bearer_token\n",
    "\n",
    "    def get_search_url_search_headers(self, api_key: str, \n",
    "                                      api_secret_key: str) -> List[str]:\n",
    "\n",
    "        key_secret = '{}:{}'.format(api_key, api_secret_key).encode('ascii')\n",
    "        b64_encoded_key = base64.b64encode(key_secret)\n",
    "        b64_encoded_key = b64_encoded_key.decode('ascii')\n",
    "\n",
    "\n",
    "        base_url = 'https://api.twitter.com/'\n",
    "        auth_url = '{}oauth2/token'.format(base_url)\n",
    "\n",
    "        auth_headers = {\n",
    "            'Authorization': 'Basic {}'.format(b64_encoded_key),\n",
    "            'Content-Type': 'application/x-www-form-urlencoded;charset=UTF-8'\n",
    "        }\n",
    "\n",
    "        auth_data = {\n",
    "            'grant_type': 'client_credentials'\n",
    "        }\n",
    "\n",
    "        auth_resp = requests.post(auth_url, headers=auth_headers, data=auth_data)\n",
    "\n",
    "        # Check status code okay\n",
    "        if auth_resp.status_code == 200:\n",
    "            access_token = auth_resp.json()['access_token']\n",
    "\n",
    "        search_headers = {\n",
    "            'Authorization': 'Bearer {}'.format(access_token)    \n",
    "        }\n",
    "\n",
    "#         'https://api.twitter.com/1.1/tweets/search/fullarchive'\n",
    "        search_url = '{}1.1/search/tweets.json'.format(base_url)\n",
    "\n",
    "        return search_url, search_headers\n",
    "\n",
    "\n",
    "    def searchTwitter(self, search_url: str, \n",
    "                      search_headers: str, \n",
    "                      search_params: Dict[str, object]) -> object:\n",
    "\n",
    "        search_resp = requests.get(search_url, \n",
    "                                   headers=search_headers, \n",
    "                                   params=search_params)\n",
    "        tweet_data = search_resp.json()\n",
    "\n",
    "        return tweet_data\n",
    "\n",
    "    def convertTweets2DF (self, tweet_data, \n",
    "                          verbose: bool = False) -> pd.DataFrame:\n",
    "        if \"statuses\" in tweet_data.keys():\n",
    "            \n",
    "            statusesList = tweet_data[\"statuses\"]\n",
    "            statusesList\n",
    "\n",
    "            tweetsList = [] # List[Dict[str, str]]\n",
    "            for ii in range(len(statusesList)):\n",
    "                if verbose:\n",
    "                    print(ii)\n",
    "                status = statusesList[ii]\n",
    "                statusesHT = {} # Dict[str, str]\n",
    "                for kk in status.keys():\n",
    "                    if verbose:\n",
    "                        print(f\"\"\"type(status[{kk}]) = {type(status[kk])}\"\"\")\n",
    "                        print(f\"\"\"status[{kk}] = {status[kk]}\"\"\")\n",
    "                    if type(status[kk]) is str or type(status[kk]) is int or type(status[kk]) is bool: \n",
    "                        statusesHT[kk] = status[kk]\n",
    "\n",
    "                tweetDF = pd.DataFrame(statusesHT, index=[0])\n",
    "                tweetsList.append(tweetDF)  \n",
    "\n",
    "            if len(tweetsList) > 0:\n",
    "                tweetsDF = pd.concat(tweetsList, \n",
    "                                     sort=False, \n",
    "                                     ignore_index=True).reset_index(drop=True)\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            print(\"no 'statuses' found in tweet_data\")\n",
    "            return None\n",
    "        \n",
    "        return tweetsDF\n",
    "\n",
    "    def getTweetsIntoDF(self,\n",
    "                        query: str, \n",
    "                        cnt: int\n",
    "                       ) -> pd.DataFrame:\n",
    "\n",
    "        search_params = {\n",
    "            'q': query,\n",
    "            'result_type': 'recent',\n",
    "            'count': cnt,\n",
    "            'lang': 'en'\n",
    "        }\n",
    "\n",
    "        tweet_data = self.searchTwitter(self.mSearchParams[\"url\"],\n",
    "                                        self.mSearchParams[\"headers\"],\n",
    "                                        search_params)\n",
    "\n",
    "        twDF = self.convertTweets2DF(tweet_data)\n",
    "        if twDF is not None:\n",
    "            twDF[\"topic\"] = query\n",
    "\n",
    "        return twDF\n",
    "\n",
    "    def plotHistogramForRetweetsByTopic(self, twDF: pd.DataFrame):\n",
    "        twDFtweets = pd.DataFrame(twDF.groupby('topic').count()['id']).reset_index()\n",
    "        print(twDFtweets)\n",
    "        twDFretweets = pd.DataFrame(twDF.groupby('topic').mean()['retweet_count']).reset_index()\n",
    "        print(twDFretweets)\n",
    "\n",
    "        import plotly.express as px\n",
    "        tDF = twDF.loc[twDF[\"retweet_count\"] > 0].copy()\n",
    "        tDF = twDF.copy()\n",
    "        tDF[\"retweet_count\"] = tDF[\"retweet_count\"].astype(\"int64\")\n",
    "        fig = px.histogram(tDF, \n",
    "                           x=\"retweet_count\", \n",
    "                           color=tDF[\"topic\"], \n",
    "                           barmode=\"group\",\n",
    "                           title=\"Retweet Counts by topic\",\n",
    "                           nbins=20               \n",
    "                          )\n",
    "        fig.show()\n",
    "        \n",
    "    def getTweetDataFromCSV(self) -> pd.DataFrame:\n",
    "        \n",
    "        dataHT: Dict[str, pd.DataFrame] = {}\n",
    "        for ff in self.mMyFiles:\n",
    "            dataHT[ff] = pd.read_csv(self.mMyDir + ff)\n",
    "\n",
    "        if len(dataHT.keys()) > 0:\n",
    "            allData = pd.concat(dataHT).reset_index(drop=True)\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        return allData\n",
    "    \n",
    "    def getBigTopic(self, topic) -> str:\n",
    "        for kk in self.mBigTopics.keys():\n",
    "            if topic in self.mBigTopics[kk]:\n",
    "                return kk\n",
    "        return None\n",
    "    \n",
    "    def splitTopicsIDbigTopics(self, allData: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Split the topic into location and topic\n",
    "        \"\"\"\n",
    "        subtopics = allData.topic.str.split(',', expand=True)\n",
    "        subtopics.columns = ['location', 'the_topic']\n",
    "        subtopics.the_topic = subtopics.the_topic.str.strip()\n",
    "        subtopics.the_topic.unique()\n",
    "\n",
    "        allDataExt = pd.concat([allData, subtopics], axis=1)\n",
    "        allDataExt.head(1)\n",
    "\n",
    "        single_topic = allDataExt.loc[allDataExt.the_topic.isnull()].copy()\n",
    "        single_topic.the_topic = single_topic.location\n",
    "        single_topic.location = None\n",
    "        single_topic.the_topic.unique()\n",
    "\n",
    "        \"\"\"\n",
    "        Get the big topic for \"the_topic\" based on self.mBigTopics\n",
    "        \"\"\"\n",
    "        single_topic[\"big_topic\"] = single_topic.the_topic.apply(lambda x: self.getBigTopic(x))\n",
    "        single_topic.big_topic.unique()\n",
    "        \n",
    "        double_topic = allDataExt.loc[~allDataExt.the_topic.isnull()].copy()\n",
    "        double_topic[\"big_topic\"] = double_topic.the_topic.apply(lambda x: self.getBigTopic(x))\n",
    "        double_topic.big_topic.unique()\n",
    "        \n",
    "        \"\"\"\n",
    "        Concatenate single-topic and double-topic entries and return the resulting DF\n",
    "        \"\"\"\n",
    "        allDataFinal = pd.concat([single_topic, double_topic]).reset_index(drop=True)\n",
    "        \n",
    "        allDataFinal.loc[(allDataFinal.location.isnull()), 'big_topic'] = \\\n",
    "            allDataFinal.loc[(allDataFinal.location.isnull())].the_topic\n",
    "\n",
    "        allDataFinal.loc[(allDataFinal.location.isnull()), 'location'] = \\\n",
    "            allDataFinal.loc[(allDataFinal.location.isnull())].the_topic\n",
    "        \n",
    "        return allDataFinal\n",
    "      \n",
    "    def getTweets(self, \n",
    "                  myFiles: List[str], \n",
    "                  myDir: str, \n",
    "                  verbose: bool = True) -> pd.DataFrame:\n",
    "        \n",
    "        tweetsDFht: Dict[str, pd.DataFrame] = {}\n",
    "        for ff in myFiles:\n",
    "            if 'crdownload' in ff or 'zip' in ff:\n",
    "                continue\n",
    "            print(ff)\n",
    "            df = pd.read_csv(myDir + ff, encoding='utf-16', sep='\\t')\n",
    "            tweetsDFht[ff] = df.copy()\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\"\"Got the tweets from {len(myFiles)} files into a hashtable of dataframes\"\"\")\n",
    "            \n",
    "        allTweetsDF = pd.concat(tweetsDFht, ignore_index=True).drop_duplicates()\n",
    "        cols = [cc.replace(' ', '_').lower() for cc in list(allTweetsDF.columns)]\n",
    "        allTweetsDF.columns = cols\n",
    "\n",
    "        allTweetsDF[\"date\"] = pd.to_datetime(allTweetsDF.alternate_date_format, format='%b %d, %Y')\n",
    "        del allTweetsDF['alternate_date_format']\n",
    "        allTweetsDF.sort_values(['date', 'time'], inplace=True)\n",
    "        allTweetsDF.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        allTweetsDF['wk'] = allTweetsDF['date'].dt.isocalendar().week\n",
    "        if verbose:\n",
    "            print(len(allTweetsDF))\n",
    "            print(allTweetsDF['wk'].isna().sum())\n",
    "        allTweetsDF['week'] = [str(ww) if ww >= 10 else '0' + str(ww) for ww in allTweetsDF['wk']]\n",
    "        allTweetsDF['year'] = allTweetsDF['date'].dt.isocalendar().year \n",
    "        allTweetsDF['year_week'] = 'w' + allTweetsDF.week.astype(str) + '_' + allTweetsDF.year.astype(str)\n",
    "        del allTweetsDF['wk']\n",
    "\n",
    "        print(f\"\"\"Got all ({len(allTweetsDF)}) tweets from {myDir}\"\"\")\n",
    "\n",
    "        if verbose:\n",
    "            display.display(allTweetsDF.head())\n",
    "\n",
    "        return allTweetsDF\n",
    "    \n",
    "    \n",
    "    def drawWordCloud(self, \n",
    "                      region: str,\n",
    "                      myTweetsDF: pd.DataFrame,\n",
    "                      column: str = 'hit_sentence',\n",
    "                      maxWords: int = 200,\n",
    "                      color: str = 'black') -> WordCloud:\n",
    "        \n",
    "        stopwords = list(STOPWORDS).copy()\n",
    "        stopwords.extend(['qt', 'rt', '@', 'df', 'http', 'https', \n",
    "                          'say', 'include', 'google', 'graphistry',\n",
    "                          'co', \"s'\", \"d'\", \"'\", 'retweet', 'etc'\n",
    "                         ])\n",
    "\n",
    "        dataset = list(myTweetsDF[column].astype(str))\n",
    "        cleaned_word = ' '.join([str(word).replace('#', '') for word in dataset\n",
    "                                if 'http' not in word\n",
    "                                    and not word.startswith('@')\n",
    "                                    and word != 'rt'\n",
    "                                ]).lower()\n",
    "        wordcloud = WordCloud(stopwords=stopwords,\n",
    "                          background_color=color,\n",
    "                          width=800,\n",
    "                          height=500\n",
    "                         ).generate(cleaned_word)\n",
    "        plt.figure(1)\n",
    "        plt.imshow(wordcloud)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"\"\"{region} word cloud (top {maxWords}) from {myTweetsDF.date.min()} to {myTweetsDF.date.max()}\"\"\")\n",
    "        #         plt.savefig('(key).png')\n",
    "        #         plt.close()\n",
    "        plt.show()       \n",
    "        return wordcloud\n",
    "    \n",
    "    # TODO: remove location words used in the meltwater query\n",
    "    def drawWordCloudByWeek(self, \n",
    "                            region: str,\n",
    "                            tweetsDF: pd.DataFrame,\n",
    "                            column: str = 'hit_sentence',\n",
    "                            maxWords: int = 200,\n",
    "                            color: str = 'white') -> WordCloud:\n",
    "        \n",
    "        if 'week' not in tweetsDF.columns and 'year' not in tweetsDF.columns:\n",
    "            tweetsDF['wk'] = tweetsDF['date'].dt.isocalendar().week \n",
    "            tweetsDF['week'] = [str(ww) if ww >= 10 else '0' + \\\n",
    "                                str(ww) for ww in allTweetsDF['wk']]\n",
    "            tweetsDF['year'] = tweetsDF['date'].dt.isocalendar().year \n",
    "            tweetsDF['year_week'] = 'w' + tweetsDF.week.astype(str) + '_' \\\n",
    "                                    + tweetsDF.year.astype(str)\n",
    "            del tweetsDF['wk']\n",
    "\n",
    "        tweetsDF['year_week'] = tweetsDF.year.astype(str) + '_' + tweetsDF.week.astype(str)\n",
    "        \n",
    "        year_weeks = sorted(list(tweetsDF.year_week.unique()))\n",
    "\n",
    "        \n",
    "        separator = \"=============================================================\"\n",
    "        for yw in year_weeks:\n",
    "            print(separator + separator)\n",
    "            mmTweetsDF = tweetsDF.loc[tweetsDF.year_week == yw]\n",
    "            self.drawWordCloud(region, mmTweetsDF, column, maxWords, color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
