{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bringing together GDP and Personal Income by US County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/agilgur/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import AxesGrid\n",
    "from sklearn import decomposition\n",
    "import statsmodels.regression.linear_model as lm\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "from typing import List, Tuple, Dict\n",
    "\n",
    "%run ./mobilityHelpers.ipynb\n",
    "\n",
    "MYDIR = \"./../../ResearchProposal/\"\n",
    "\n",
    "myFiles = os.listdir(MYDIR)\n",
    "\n",
    "gdpFile = MYDIR + \"bea_gov/gdp/gdp_ready_to_analyze.csv\"\n",
    "piFile = MYDIR + \"bea_gov/personal_income/personal_income_ready_to_analyze.csv\"\n",
    "hhiFile = MYDIR + \"income_inequality/census_income_by_county/hh_income__census_data.csv\"\n",
    "populationFile = MYDIR + \"population_dynamics/census_population_data_2010_2019.csv\"\n",
    "suicideFile = MYDIR + \"suicide/multiple_causes_of_death__suicide.csv\"\n",
    "employmentFile = MYDIR + \"unemployment/employment_by_county_state_year.csv\"\n",
    "stateAbbrevFile = MYDIR + \"state_abbreviations.csv\"\n",
    "myfiles = {\"gdp\": gdpFile,\n",
    "           \"pi\": piFile,\n",
    "           \"hhi\": hhiFile,\n",
    "           \"pop\": populationFile,\n",
    "           \"sc\": suicideFile,\n",
    "           \"emp\": employmentFile,\n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mobilityMLhelpers:\n",
    "    def __init__(self, \n",
    "                 myFiles: Dict[str, str] = myfiles,\n",
    "                 ctv_cutoff: float = 0.055):\n",
    "        self.mCTVcutoff = ctv_cutoff\n",
    "        self.mFiles = myfiles\n",
    "        \n",
    "        self.mStateAbbreviationsDF = pd.read_csv(stateAbbrevFile)\n",
    "       \n",
    "    def splitDataIntoTrainTest(self, myPCAdata: pd.DataFrame) -> Tuple[float]:\n",
    "        y = myPCAdata[\"NETMIG\"].copy()\n",
    "        X = myPCAdata[[cc for cc in myPCAdata.columns if \"NETMIG\" not in cc]].copy()\n",
    "\n",
    "        # Splitting the dataset into the Training set and Test set\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "        sc = StandardScaler()\n",
    "        X_train = sc.fit_transform(X_train)\n",
    "        X_test = sc.transform(X_test)\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Fit RFR given the n_est; test the regressor by computing its RSq \n",
    "    on unseen data and plotting RFR prediction with unseen data\n",
    "    \"\"\"\n",
    "    def fitRFR(self, n_est, X_train, y_train, X_test, y_test, showplots: bool = True):\n",
    "        myRFR = RandomForestRegressor(n_estimators=n_est)\n",
    "        myRFR.fit(X_train, y_train)\n",
    "        myRFR.feature_importances_\n",
    "\n",
    "        y_predict_test = myRFR.predict(X=X_test)\n",
    "\n",
    "        error_term = [list(y_test)[ii] - list(y_predict_test)[ii] for ii in range(len(list(y_test)))]\n",
    "        var_error = np.var(error_term)\n",
    "        RSq = 1.0 - (var_error) / np.var(y_test)\n",
    "\n",
    "        print(f\"\"\"RSq = {RSq: .4f}\"\"\")\n",
    "        if showplots:\n",
    "            fig, ax = plt.subplots(figsize=(10, 8))\n",
    "            plt.scatter(y_test, y_predict_test)\n",
    "            ax.set_xlabel(\"y_test\")\n",
    "            ax.set_ylabel(\"RFR(\" + str(n_est) + \") predict\")\n",
    "            ax.set_title(\"RFR with \" + str(n_est) + \" estimators\")\n",
    "            plt.show()\n",
    "\n",
    "        return(RSq, myRFR)\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    Run PCA; cut off PCs with explained_variance_ >= min_explained_var\n",
    "    \"\"\"\n",
    "    def runPCA(self, min_explained_var, myPCAdata, verbose: bool = True):\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = \\\n",
    "            self.splitDataIntoTrainTest(myPCAdata)\n",
    "        \n",
    "        pca = PCA()\n",
    "        X_train_pca = pca.fit_transform(X_train)\n",
    "        X_test_pca = pca.transform(X_test)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"------------PCA Explained Variance------------\")\n",
    "            print(pca.explained_variance_)\n",
    "            print(\"------------PCA Explained Variance Ratio------------\")\n",
    "            print(pca.explained_variance_ratio_)\n",
    "            \n",
    "        \"\"\"\n",
    "        Rerun PCA for n_components = number of PCs with \n",
    "        explained_variance_ >= min_explained_var\n",
    "        \"\"\"\n",
    "        \n",
    "        ppc = [ee for ee in list(pca.explained_variance_ratio_) if ee > EXPL_VAR]\n",
    "        if verbose:\n",
    "            print(f\"\"\"------------Components with Explained Variance Ratio > {EXPL_VAR} ------------\"\"\")\n",
    "            print(len(ppc))\n",
    "\n",
    "        pca = PCA(n_components=len(ppc))\n",
    "        X_train = pca.fit_transform(X_train)\n",
    "        X_test = pca.transform(X_test)\n",
    "        if verbose:\n",
    "            print(\"------------PCA Explained Variance------------\")\n",
    "            print(pca.explained_variance_)\n",
    "            print(\"------------PCA Explained Variance Ratio------------\")\n",
    "            print(pca.explained_variance_ratio_)\n",
    "    \n",
    "        return pca, X_train, X_test, y_train, y_test\n",
    "    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Details in \n",
    "    https://stackoverflow.com/questions/31909945/obtain-eigen-values-and-vectors-from-sklearn-pca/31941631#31941631\n",
    "    \n",
    "    Apply PCA; select the primary principal components:\n",
    "    (a) outliers in eigenvalues (default);\n",
    "    (b) PCs with eigenvalues greater than a given tolerance % of max eigenvalue; \n",
    "    (c) or  with the lowest frequency on the histogram.\n",
    "    \n",
    "    Note: the outlier-based selection will only work if we have more than 10-15 PCs.  \n",
    "    This method is sensitive to number of PCs.\n",
    "    \"\"\"\n",
    "    def applyPCA(self, \n",
    "                 gdpiData: pd.DataFrame,\n",
    "                 cols: List[str],\n",
    "                 n_comps: int,\n",
    "                 identify_outliers: bool = True,\n",
    "                 beta_outliers: float = 1.5,\n",
    "                 tolerance: float = -1.0,\n",
    "                 remove_most_frequent: bool = False,\n",
    "                 \n",
    "                 verbose: bool = False,\n",
    "                 showplots: bool = False) -> Tuple[pd.DataFrame, List[float]]:\n",
    "\n",
    "\n",
    "        data = myDF[cols]\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\"\"We have {len(cols)} population-dynamics variables \"\"\"\n",
    "              f\"\"\"to use for modeling net migration into each county\"\"\")\n",
    "\n",
    "            print(f\"\"\"We are going to reduce it by identifying the\"\"\"\n",
    "                  f\"\"\"primary components.  We will start with {n_comps}\"\"\")\n",
    "        \n",
    "        \"\"\"Normalize by StDev:\"\"\"\n",
    "        data /= np.std(data, axis=0)\n",
    "\n",
    "        pcaMdl = decomposition.PCA(n_components=n_comps)\n",
    "        pcaMdl.fit(data)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Eigenvectors:\")\n",
    "            print(pcaMdl.components_)\n",
    "            print(\"Eigenvalues:\")\n",
    "            print(pcaMdl.explained_variance_)\n",
    "\n",
    "        eigenvalues = pcaMdl.explained_variance_\n",
    "        transformed = pcaMdl.transform(data)\n",
    "\n",
    "        for cc in range(n_comps):\n",
    "            myDF[\"pc_\" + str(cc)] = [transformed[ii][cc] for ii in range(len(transformed))]\n",
    "\n",
    "        \"\"\"Identify Outliers in Eigenvalues\"\"\"\n",
    "        if identify_outliers:\n",
    "            print(f\"\"\"Of these {n_comps}, we will select \"\"\"\n",
    "                  f\"\"\"the EigenValue outliers, to avoid overfitting the model\"\"\")\n",
    "            ax = plt.subplot()\n",
    "            ax.boxplot(eigenvalues, vert=False)\n",
    "            ax.set_title(\"Eigenvalues of the principal components\", fontsize=14)\n",
    "            plt.show()\n",
    "            \n",
    "            \"\"\"\n",
    "            Select primary components as outliers in Eigenvalues\n",
    "            \"\"\"\n",
    "            qrtls = np.percentile(eigenvalues, q=(25.0, 75.0))\n",
    "            qrtls\n",
    "            iqr = max(qrtls) - min(qrtls)\n",
    "            high_bound = max(qrtls) + beta_outliers * iqr\n",
    "            high_bound\n",
    "\n",
    "            primary_components = [ee for ee in eigenvalues if ee >= high_bound]\n",
    "            print(f\"\"\"In this case, primary components are ones \"\"\"\n",
    "                  f\"\"\"with eigenvalues >= {high_bound}\"\"\")\n",
    "        else:\n",
    "            primary_components = [ee for ee in eigenvalues if ee >= max(qrtls)]\n",
    "            print(f\"\"\"In this case, primary components are ones \"\"\"\n",
    "                  f\"\"\"with eigenvalues >= {max(qrtls)}\"\"\")\n",
    "             \n",
    "        print(f\"\"\"We have {len(primary_components)} primary principal components:\"\"\")\n",
    "        for ii in range(len(primary_components)):\n",
    "            print(f\"\"\"{ii}: {primary_components[ii]:.3f}\"\"\")\n",
    "            \n",
    "        if tolerance > 0:\n",
    "            maxPC = max(eigenvalues)\n",
    "            print(f\"\"\"Removing PCs with eigenvalues < {(100.0*tolerance): .3f}% of {maxPC}\"\"\")\n",
    "            primary_components = [pc for pc in eigenvalues if pc >= tolerance * maxPC]\n",
    "            \n",
    "        if remove_most_frequent:\n",
    "            ax = plt.subplot()\n",
    "            myHist = ax.hist(eigenvalues)\n",
    "            if verbose:\n",
    "                print(myHist[0])\n",
    "                print(myHist[1])\n",
    "            counts = list(myHist[0])\n",
    "            maxCountIndex = counts.index(max(counts))\n",
    "            maxCountEigenvalue = myHist[1][maxCountIndex]\n",
    "            \n",
    "            primary_components = [pp for pp in eigenvalues if pp > maxCountEigenvalue]\n",
    "            \n",
    "        return myDF, list(primary_components)\n",
    "    \n",
    "    \"\"\"\n",
    "    Apply linear regression\n",
    "    \"\"\"\n",
    "    def applyLinRegr(self, \n",
    "                     pcPopDF: pd.DataFrame, \n",
    "                     year: int,\n",
    "                     primary_components: List[float],\n",
    "                     verbose: bool = True,\n",
    "                     showplots: bool = True,\n",
    "                    ) -> pd.DataFrame:\n",
    "        print(year)\n",
    "\n",
    "        yVar = \"NETMIG\" + str(year)\n",
    "        frmla = yVar + \" ~ \"\n",
    "        frmla\n",
    "\n",
    "        rhs = \" + \".join([\"pc_\" + str(ii) for ii in range (len(primary_components))])\n",
    "        rhs\n",
    "\n",
    "        frmla += rhs\n",
    "        frmla\n",
    "        \n",
    "        linMdl = lm.OLS.from_formula(formula=frmla, data = pcPopDF)\n",
    "        res = linMdl.fit()\n",
    "\n",
    "        if verbose:\n",
    "            print(res.summary())\n",
    "        \n",
    "        pcPopDF[yVar + \"_LM\"] = res.predict()\n",
    "        \n",
    "        if showplots:\n",
    "            ax = pcPopDF.plot.scatter(x=yVar, \n",
    "                                 y=yVar + \"_LM\", \n",
    "                                 figsize=(10, 6))\n",
    "            ax.set_title(yVar + \" and linear regression prediction\",\n",
    "                         fontsize=16\n",
    "                        )\n",
    "            ax.set_xlabel(yVar, fontsize=14)\n",
    "            ax.set_ylabel(yVar + \" model prediction\", fontsize=14)\n",
    "            plt.show()\n",
    "            \n",
    "        return pcPopDF\n",
    "    \n",
    "    \"\"\"\n",
    "    Fit random forest regression to get the importance of each of the primary components\n",
    "    \"\"\"\n",
    "    def fitRFRdf(self, \n",
    "               pcPopDF: pd.DataFrame, \n",
    "               xVars: List[str] = [],\n",
    "               yVarBase: str = \"NETMIG\",\n",
    "               year: int = 2010,\n",
    "               ctv_cutoff: float = 0.055,\n",
    "               n_rfr_trees: int = 100,\n",
    "               verbose: bool = False,\n",
    "              ) -> Tuple[List[str], pd.DataFrame]:\n",
    "        \n",
    "        # create regressor object \n",
    "        regressor = RandomForestRegressor(n_estimators=n_rfr_trees, random_state = 0) \n",
    "\n",
    "        # fit the regressor with x and y data \n",
    "        if yVarBase is None or len(yVarBase) == 0:\n",
    "            yVarBase = \"NETMIG\"\n",
    "            \n",
    "        yVar = yVarBase + str(year)\n",
    "        \n",
    "        x = pcPopDF[xVars]\n",
    "        y = pcPopDF[yVar]\n",
    "        rfr = regressor.fit(x, y) \n",
    "    \n",
    "        pcPopDF[yVar + \"_rfr_predict\"] = regressor.predict(pcPopDF[xVars])\n",
    "        importances = list(rfr.feature_importances_)\n",
    "        importances\n",
    "\n",
    "        important_features = \\\n",
    "            [ii for ii in range(len(importances)) if importances[ii] > ctv_cutoff]\n",
    "\n",
    "        importantXs = [xVars[impfeat] for impfeat in important_features]\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\"\"Feature Importances (contributions to variance):\"\"\")\n",
    "\n",
    "            \"\"\"sort xVars in order of importances\"\"\"\n",
    "            ctvDict = {} # Dict[str: int]\n",
    "            for ii in range(len(importances)):\n",
    "                ctvDict[xVars[ii]] = importances[ii]\n",
    "                \n",
    "            ctvlist = sorted(ctvDict.items(), key=lambda x: x[1], reverse=True)\n",
    "            for kk in ctvlist:\n",
    "                print(f\"\"\"{kk[0]}: {kk[1]: .3f}\"\"\")\n",
    "                \n",
    "            print(f\"\"\"Identified important features (CTV cutoff = {ctv_cutoff})\\n\"\"\")\n",
    "\n",
    "            \n",
    "            for kk in ctvlist:\n",
    "                if kk[0] in importantXs:\n",
    "                    print(f\"\"\"{kk[0]}: {kk[1]: .3f}\"\"\")\n",
    "                    \n",
    "            print(f\"\"\"\\nRFR R^2 with identified features: \"\"\"\n",
    "                  f\"\"\"{sum([importances[ii] for ii in important_features]): .3f}\"\"\")\n",
    "\n",
    "#         importantXs = [\"pc_\" + str(ii) for ii in important_features]\n",
    "        \n",
    "    \n",
    "        return [xVars[impfeat] for impfeat in important_features], pcPopDF\n",
    "    \n",
    "    def applyLinearRegression(self,\n",
    "                              pcPopDF: pd.DataFrame,\n",
    "                              importantXs: List[str],\n",
    "                              yVarBase: str = \"NETMIG\",\n",
    "                              year: int = 2010,\n",
    "                              verbose: bool = True,\n",
    "                              showplots: bool = True,\n",
    "                             ) -> pd.DataFrame:\n",
    "        \n",
    "        yVar = yVarBase + str(year)\n",
    "        frmla = yVar + \" ~ \"\n",
    "        print(frmla)\n",
    "\n",
    "        rhs = \" + \".join(importantXs)\n",
    "        print(rhs)\n",
    "        \n",
    "\n",
    "        frmla += rhs\n",
    "        print(frmla)\n",
    "\n",
    "        verbose=True\n",
    "        showplots=True\n",
    "\n",
    "        linMdl = lm.OLS.from_formula(formula=frmla, data = pcPopDF)\n",
    "        res = linMdl.fit()\n",
    "\n",
    "        if verbose:\n",
    "            print(res.summary())\n",
    "\n",
    "        pcPopDF[yVar + \"_LM\"] = res.predict()\n",
    "\n",
    "        if showplots:\n",
    "            ax = pcPopDF.plot.scatter(x=yVar, \n",
    "                                 y=yVar + \"_LM\", \n",
    "                                 figsize=(10, 6))\n",
    "            ax.set_title(yVar + \" and linear regression prediction\",\n",
    "                         fontsize=16\n",
    "                        )\n",
    "            ax.set_xlabel(yVar, fontsize=14)\n",
    "            ax.set_ylabel(yVar + \" model prediction\", fontsize=14)\n",
    "            plt.show()\n",
    "\n",
    "    def plot_importances(self, forest, X, y):\n",
    "        plt.rcParams.update({\"font.size\": 16})\n",
    "        importances = forest.feature_importances_\n",
    "        std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "                     axis=0)\n",
    "        indices = np.argsort(-1 * importances)\n",
    "\n",
    "        # Plot the feature importances of the forest\n",
    "        plt.figure(figsize=(15, 8))\n",
    "        plt.title(\"Feature importances\")\n",
    "        plt.bar(range(X.shape[1]), importances[indices],\n",
    "               color=\"r\", yerr=std[indices], align=\"center\")\n",
    "        # If you want to define your own labels,\n",
    "        # change indices to a list of labels on the following line.\n",
    "        plt.xticks(range(X.shape[1]), indices)\n",
    "        plt.xlim([-1, X.shape[1]])\n",
    "        plt.show()  \n",
    "        \n",
    "    def plot_explained_variance_pca(self, pca):\n",
    "        plt.rcParams.update({\"font.size\": 16})\n",
    "        \n",
    "        plt.figure(figsize=(15, 8))\n",
    "        plt.title(\"Explained Variance Ratio\")\n",
    "        plt.bar(range(len(list(pca.explained_variance_ratio_))), \n",
    "                pca.explained_variance_ratio_,\n",
    "               color=\"r\", align=\"center\")\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
